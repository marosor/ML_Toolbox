{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, SelectFromModel, RFE, SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion: super_selector \n",
    "\n",
    "Esta función debe recibir como argumento un dataframe de features \"dataset\", un argumento \"target_col\" (que puede hacer referencia a una feature numérica o categórica) que puede ser \"\", un argumento \"selectores\" de tipo diccionario que puede estar vacío, y un argumento \"hard_voting\" como una lista vacía. \n",
    "\n",
    "CAUSISTICA y funcionamiento:\n",
    "\n",
    "* Si target_col no está vacío y es un columna válidad del dataframe, la función comprobará el valor de \"selectores\":\n",
    "    * Si \"selectores\" es un diccionario vacío o None:\n",
    "        La fución devuelve una lista con todas las columnas del dataframe que no sean el target, tengan un valor de cardinalidad diferente del 99.99% (no sean índices) y no tengan un único valor.\n",
    "    * Si \"selectores\" no es un diccionario vacío, espera encontrar las siguientes posibles claves (y actúa en consecuencia):  \n",
    "        \"KBest\": Tendrá como valor el número de features a seleccionar aplicando un KBest. La función debe crear una lista con las features obtenidas de emplear un SelectKBest con ANOVA.  \n",
    "        \"FromModel\": Tendrás como valores una lista con dos elementos, el primero la instancia de un modelo de referencia y el segundo un valor entero o compatible con el argumento \"threshold\" de SelectFromModel de sklearn. En este caso la función debe crear un a lista con las features obtenidas de aplicar un SelectFromModel con el modelo de referencia, y utilizando \"threshold\" con el valor del segundo elemento si este no es un entero. En este caso, cuando sea un entero, usarás SelectFromModel con los argumentos \"max_features\" igual al valor del segundo elemento y \"threshold\" igual a -np.inf. (Esto hace que se seleccionen \"max_features\" features)  \n",
    "        \"RFE\": Tendrá como valor una tupla con tres elementos. El primero será un modelo instanciado, el segundo elemento determina el número de features a seleccionar y el tercero el step a aplicar. Serán los tres argumentos del RFE de sklearn que usará la función para generar una lista de features.  \n",
    "        \"SFS\": Tendrá como valor un tupla con 2 elementos, el modelo de referencia instanciado y el numero de featureas a alcanzar. Esta vez la función empleará un SFS para obtener las lista de features seleccionadas.\n",
    "\n",
    "* La función debe devolver tantas listas seleccionadas como claves en el diccionario de selectores y una adicional con el resultado de aplicar un hard voting a las listas obtenidas de aplicar el diccionario \"selectores\" y las que contenga \"hard_voting\", en caso de que \"hard_voting\" contenga una o más listas. La función devolverá un diccionario con claves equivalentes a las de selectores pero con la lista correspondiente asignada a cada clave y una adicional \"hard_voting\" caso de que \"hard_voting\" como argumento no sea una lista vacía.\n",
    "\n",
    "Ejemplo:\n",
    "\n",
    "```python\n",
    "selectores = {\n",
    "    \"KBest\": 5,\n",
    "    \"FromModel\": [RandomForestClassifier(),5],\n",
    "    \"RFE\": [LogisticRegression(),5,1]\n",
    "}\n",
    "super_selector(train_set_titanic, target_col = \"Survived\", selectores = selectores, hard_voting = [\"Pclass\",\"who\",\"embarked_S\",\"fare\",\"age\"])\n",
    "\n",
    "```\n",
    "\n",
    "Devolvera un diccionario del tipo: \n",
    "```python\n",
    "{\n",
    "    \"KBest\": [lista de features obtenidas con un SelectKBest(f_classif, k=5) con fit a train_set_titanic y target_col, sin la target_col en train_set_titanic, claro],\n",
    "    \"FromModel\": [lista de features obtenidas de aplicar un SelecFromModel con el RandomForestClassfier y max_features = 5 y threshold = -np.inf],\n",
    "    \"RFE\": [lista de features obtenidas de un RFE con argumentos el LogisticRegressor, n_features_to_select = 5, y step = 1],\n",
    "    \"hard_voting\": [lista con las len(hard_voting) features con más votos entre las cuatro listas]\n",
    "}\n",
    "```\n",
    "NOTA: Si hard_voting esta a [], la función sigue devolviendo el hard_voting pero sólo con las listas creadas internamente (si hay una sola también), es decir que la función siempre devuelve al menos dos listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_selector(dataset, target_col = \"\", selectores = None, hard_voting = []):\n",
    "\n",
    "    \"\"\"\n",
    "    Función que selecciona features de un dataframe utilizando varios métodos y realiza un hard voting entre las listas seleccionadas\n",
    "    \n",
    "    Argumentos:\n",
    "    dataset (pd.DataFrame): DataFrame con las features y el target\n",
    "    target_col (str): Columna objetivo en el dataset. Puede ser numérica o categórica\n",
    "    selectores (dict): Diccionario con los métodos de selección a utilizar. Puede contener las claves \"KBest\", \"FromModel\", \"RFE\" y \"SFS\"\n",
    "    hard_voting (list): Lista de features para incluir en el hard voting\n",
    "\n",
    "    Retorna:\n",
    "    dict: Diccionario con las listas de features seleccionadas por cada método y una lista final por hard voting\n",
    "    \"\"\"\n",
    "    \n",
    "    if selectores is None:\n",
    "        selectores = {}\n",
    "    \n",
    "    features = dataset.drop(columns = [target_col]) if target_col else dataset\n",
    "    target = dataset[target_col] if target_col else None\n",
    "    \n",
    "    result = {}\n",
    "\n",
    "    # Caso en que selectores es vacío o None\n",
    "    if target_col and target_col in dataset.columns:\n",
    "        if not selectores:\n",
    "            filtered_features = [col for col in features.columns if\n",
    "                                 (features[col].nunique() / len(features) < 0.9999) and\n",
    "                                 (features[col].nunique() > 1)]\n",
    "            result[\"all_features\"] = filtered_features\n",
    "\n",
    "    # Aplicación de selectores si no es vacío\n",
    "    if selectores:\n",
    "        if \"KBest\" in selectores:\n",
    "            k = selectores[\"KBest\"]\n",
    "            selector = SelectKBest(score_func = f_classif, k = k)\n",
    "            selector.fit(features, target)\n",
    "            selected_features = features.columns[selector.get_support()].tolist()\n",
    "            result[\"KBest\"] = selected_features\n",
    "\n",
    "        if \"FromModel\" in selectores:\n",
    "            model, threshold_or_max = selectores[\"FromModel\"]\n",
    "            if isinstance(threshold_or_max, int):\n",
    "                selector = SelectFromModel(model, max_features = threshold_or_max, threshold = -np.inf)\n",
    "            else:\n",
    "                selector = SelectFromModel(model, threshold = threshold_or_max)\n",
    "            selector.fit(features, target)\n",
    "            selected_features = features.columns[selector.get_support()].tolist()\n",
    "            result[\"FromModel\"] = selected_features\n",
    "\n",
    "        if \"RFE\" in selectores:\n",
    "            model, n_features, step = selectores[\"RFE\"]\n",
    "            selector = RFE(model, n_features_to_select = n_features, step = step)\n",
    "            selector.fit(features, target)\n",
    "            selected_features = features.columns[selector.get_support()].tolist()\n",
    "            result[\"RFE\"] = selected_features\n",
    "\n",
    "        if \"SFS\" in selectores:\n",
    "            model, k_features = selectores[\"SFS\"]\n",
    "            sfs = SequentialFeatureSelector(model, n_features_to_select = k_features, direction = \"forward\")\n",
    "            sfs.fit(features, target)\n",
    "            selected_features = features.columns[sfs.get_support()].tolist()\n",
    "            result[\"SFS\"] = selected_features\n",
    "\n",
    "    # Hard Voting\n",
    "    if hard_voting or selectores:\n",
    "        voting_features = []\n",
    "        if \"hard_voting\" not in result:\n",
    "            voting_features = hard_voting.copy()\n",
    "        for key in result:\n",
    "            voting_features.extend(result[key])\n",
    "\n",
    "        feature_counts = pd.Series(voting_features).value_counts()\n",
    "        hard_voting_result = feature_counts[feature_counts > 1].index.tolist()\n",
    "        \n",
    "        result[\"hard_voting\"] = hard_voting_result if hard_voting_result else list(feature_counts.index)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataframe\n",
    "titanic = pd.read_csv(\"./data/titanic.csv\")\n",
    "\n",
    "# Transformar variables categóricas\n",
    "titanic = pd.get_dummies(titanic, drop_first = True)\n",
    "\n",
    "# Definir la columna objetivo\n",
    "target_col = 'alive_yes'\n",
    "\n",
    "# Separar las features y el target\n",
    "features = titanic.drop(columns = [target_col])\n",
    "target = titanic[target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'KBest': ['adult_male', 'sex_male', 'class_Third', 'who_man', 'who_woman'],\n",
       " 'FromModel': ['age', 'fare', 'adult_male', 'sex_male', 'who_man'],\n",
       " 'RFE': ['age', 'fare', 'adult_male', 'class_Third', 'who_man'],\n",
       " 'SFS': ['fare',\n",
       "  'adult_male',\n",
       "  'class_Second',\n",
       "  'class_Third',\n",
       "  'embark_town_Queenstown'],\n",
       " 'hard_voting': ['adult_male',\n",
       "  'class_Third',\n",
       "  'who_man',\n",
       "  'fare',\n",
       "  'sex_male',\n",
       "  'age']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectores = {\n",
    "    \"KBest\" : 5,\n",
    "    \"FromModel\" : (RandomForestClassifier(), 5),\n",
    "    \"RFE\" : (RandomForestClassifier(), 5, 1),\n",
    "    \"SFS\" : (RandomForestClassifier(), 5)\n",
    "}\n",
    "\n",
    "resultados = super_selector(titanic, target_col = target_col, selectores = selectores)\n",
    "resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

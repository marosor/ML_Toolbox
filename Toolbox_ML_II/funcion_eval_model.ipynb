{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import f_oneway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion: eval_model\n",
    "\n",
    "Esta función debe recibir un target, unas predicciones para ese target, un argumento que determine si el problema es de regresión o clasificación y una lista de métricas:\n",
    "* Si el argumento dice que el problema es de regresión, la lista de métricas debe admitir las siguientes etiquetas RMSE, MAE, MAPE, GRAPH.\n",
    "* Si el argumento dice que el problema es de clasificación, la lista de métrica debe admitir, ACCURACY, PRECISION, RECALL, CLASS_REPORT, MATRIX, MATRIX_RECALL, MATRIX_PRED, PRECISION_X, RECALL_X. En el caso de las _X, X debe ser una etiqueta de alguna de las clases admitidas en el target.\n",
    "\n",
    "Funcionamiento:\n",
    "* Para cada etiqueta en la lista de métricas:\n",
    "- RMSE, debe printar por pantalla y devolver el RMSE de la predicción contra el target.\n",
    "- MAE, debe pintar por pantalla y devolver el MAE de la predicción contra el target. \n",
    "- MAPE, debe pintar por pantalla y devolver el MAPE de la predcción contra el target. Si el MAPE no se pudiera calcular la función debe avisar lanzando un error con un mensaje aclaratorio\n",
    "- GRAPH, la función debe pintar una gráfica comparativa (scatter plot) del target con la predicción\n",
    "- ACCURACY, pintará el accuracy del modelo contra target y lo retornará.\n",
    "- PRECISION, pintará la precision media contra target y la retornará.\n",
    "- RECALL, pintará la recall media contra target y la retornará.\n",
    "- CLASS_REPORT, mostrará el classification report por pantalla.\n",
    "- MATRIX, mostrará la matriz de confusión con los valores absolutos por casilla.\n",
    "- MATRIX_RECALL, mostrará la matriz de confusión con los valores normalizados según el recall de cada fila (si usas ConfussionMatrixDisplay esto se consigue con normalize = \"true\")\n",
    "- MATRIX_PRED, mostrará la matriz de confusión con los valores normalizados según las predicciones por columna (si usas ConfussionMatrixDisplay esto se consigue con normalize = \"pred\")\n",
    "- PRECISION_X, mostrará la precisión para la clase etiquetada con el valor que sustituya a X (ej. PRECISION_0, mostrará la precisión de la clase 0)\n",
    "- RECALL_X, mostrará el recall para la clase etiquetada co nel valor que sustituya a X (ej. RECALL_red, mostrará el recall de la clase etiquetada como \"red\")\n",
    "\n",
    "NOTA1: Como puede que la función devuelva varias métricas, debe hacerlo en una tupla en el orden de aparición de la métrica en la lista que se le pasa como argumento. Ejemplo si la lista de entrada es [\"GRAPH\",\"RMSE\",\"MAE\"], la fución pintará la comparativa, imprimirá el RMSE y el MAE (da igual que lo haga antes de dibujar la gráfica) y devolverá una tupla con el (RMSE,MAE) por ese orden.\n",
    "NOTA2: Una lista para clasificación puede contener varias PRECISION_X y RECALL_X, pej [\"PRECISION_red\",\"PRECISION_white\",\"RECALL_red\"] es una lista válida, tendrá que devolver la precisión de \"red\", la de \"white\" y el recall de \"red\". Si algunas de las etiquetas no existe debe arrojar ese error y detener el funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(target, predicciones, tipo_de_problema, metricas):\n",
    "\n",
    "    \"\"\"\n",
    "    Función que evalua un modelo de Machine Learning utilizando diferentes métricas para problemas de regresión o clasificación\n",
    "\n",
    "    Argumentos:\n",
    "    target (tipo array): Valores del target\n",
    "    predicciones (tipo array): Valores predichos por el modelo\n",
    "    tipo_de_problema (str): Puede ser de regresión o clasificación\n",
    "    metricas (list): Lista de métricas a calcular:\n",
    "                     Para problemas de regresión: \"RMSE\", \"MAE\", \"MAPE\", \"GRAPH\"\n",
    "                     Para problemas de clasificación: \"ACCURACY\", \"PRECISION\", \"RECALL\", \"CLASS_REPORT\", \"MATRIX\", \"MATRIX_RECALL\", \"MATRIX_PRED\", \"PRECISION_X\", \"RECALL_X\"\n",
    "\n",
    "    Retorna:\n",
    "    tupla: Devuelve una tupla con los resultados de las métricas especificadas\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Regresión\n",
    "\n",
    "    if tipo_de_problema == \"regresion\":\n",
    "\n",
    "        for metrica in metricas:\n",
    "            \n",
    "            if metrica == \"RMSE\":\n",
    "                rmse = np.sqrt(mean_squared_error(target, predicciones))\n",
    "                print(f\"RMSE: {rmse}\")\n",
    "                results.append(rmse)\n",
    "            \n",
    "            elif metrica == \"MAE\":\n",
    "                mae = mean_absolute_error(target, predicciones)\n",
    "                print(f\"MAE: {mae}\")\n",
    "                results.append(mae)\n",
    "\n",
    "            elif metrica == \"MAPE\":\n",
    "                try:\n",
    "                    mape = np.mean(np.abs((target - predicciones) / target)) * 100\n",
    "                    print(f\"MAPE: {mape}\")\n",
    "                    results.append(mape)\n",
    "                except ZeroDivisionError:\n",
    "                    raise ValueError(\"No se puede calcular el MAPE cuando hay valores en el target iguales a cero\")\n",
    "           \n",
    "            elif metrica == \"GRAPH\":\n",
    "                plt.scatter(target, predicciones)\n",
    "                plt.xlabel(\"Real\")\n",
    "                plt.ylabel(\"Predicción\")\n",
    "                plt.title(\"Gráfico de Dispersión: Valores reales VS Valores predichos\")\n",
    "                plt.show()\n",
    "\n",
    "     # Clasificación\n",
    "                \n",
    "    elif tipo_de_problema == \"clasificacion\":\n",
    "\n",
    "        for metrica in metricas:\n",
    "            \n",
    "            if metrica == \"ACCURACY\":\n",
    "                accuracy = accuracy_score(target, predicciones)\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                results.append(accuracy)\n",
    "\n",
    "            elif metrica == \"PRECISION\":\n",
    "                precision = precision_score(target, predicciones, average = \"macro\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                results.append(precision)\n",
    "\n",
    "            elif metrica == \"RECALL\":\n",
    "                recall = recall_score(target, predicciones, average = \"macro\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                results.append(recall)\n",
    "\n",
    "            elif metrica == \"CLASS_REPORT\":\n",
    "                print(\"Classification Report:\")\n",
    "                print(classification_report(target, predicciones))\n",
    "\n",
    "            elif metrica == \"MATRIX\":\n",
    "                print(\"Confusion Matrix (Absolute Values):\")\n",
    "                print(confusion_matrix(target, predicciones))\n",
    "\n",
    "            elif metrica == \"MATRIX_RECALL\":\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(target, predicciones))\n",
    "                disp.plot(normalize = \"true\")\n",
    "                plt.title(\"Confusion Matrix (Normalized by Recall)\")\n",
    "                plt.show()\n",
    "\n",
    "            elif metrica == \"MATRIX_PRED\":\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(target, predicciones))\n",
    "                disp.plot(normalize = \"pred\")\n",
    "                plt.title(\"Confusion Matrix (Normalized by Prediction)\")\n",
    "                plt.show()\n",
    "\n",
    "            elif \"PRECISION_\" in metrica:\n",
    "                class_label = metrica.split(\"_\")[-1]\n",
    "                try:\n",
    "                    precision_class = precision_score(target, predicciones, labels = [class_label])\n",
    "                    print(f\"Precisión para la clase {class_label}: {precision_class}\")\n",
    "                    results.append(precision_class)\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"La clase {class_label} no está presente en las predicciones\")\n",
    "                \n",
    "            elif \"RECALL_\" in metrica:\n",
    "                class_label = metrica.split(\"_\")[-1]\n",
    "                try:\n",
    "                    recall_class = recall_score(target, predicciones, labels = [class_label])\n",
    "                    print(f\"Recall para la clase {class_label}: {recall_class}\")\n",
    "                    results.append(recall_class)\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"La clase {class_label} no está presente en las predicciones\")\n",
    "                \n",
    "    # Si no es regresión o clasificación\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"El tipo de problema debe ser de regresión o clasificación\")\n",
    "\n",
    "    return tuple(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Función #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion: plot_features_num_classification\n",
    "\n",
    "Esta función recibe un dataframe, una argumento \"target_col\" con valor por defecto \"\", una lista de strings (\"columns\") cuyo valor por defecto es la lista vacía,  y un argumento (\"pvalue\") con valor 0.05 por defecto.\n",
    "\n",
    "Si la lista no está vacía, la función pintará una pairplot del dataframe considerando la columna designada por \"target_col\" y aquellas incluidas en \"column\" que cumplan el test de ANOVA para el nivel 1-pvalue de significación estadística. La función devolverá los valores de \"columns\" que cumplan con las condiciones anteriores. Ojo, se espera que las columnas sean numéricas. El pairplot utilizar como argumento de hue el valor de target_col.\n",
    "\n",
    "Si la lista está vacía, entonces la función igualará \"columns\" a las variables numéricas del dataframe y se comportará como se describe en el párrafo anterior.\n",
    "\n",
    "EXTRA_1: Se valorará adicionalmente el hecho de que si el número de valores posibles de target_Col se superior a 5, se usen diferentes pairplot diferentes, en cuyo caso pintará un pairplot por cada 5 valores de target posibles.\n",
    "\n",
    "EXTRA_2: Se valorará adicionalmente el hecho de que si la lista de columnas a pintar es grande se pinten varios pairplot con un máximo de cinco columnas en cada pairplot (siendo siempre una de ellas la indicada por \"target_col\")\n",
    "\n",
    "De igual manera que en la función descrita anteriormente deberá hacer un check de los valores de entrada y comportarse como se describe en el último párrafo de la función `get_features_num_classification`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features_num_classification(dataframe, target_col=\"\", columns=None, pvalue=0.05):\n",
    "    \"\"\"\n",
    "    Genera pairplots para visualizar la relación entre las columnas numéricas de un dataframe y una columna objetivo, \n",
    "    filtrando aquellas columnas que pasan una prueba de ANOVA según un nivel de significación especificado.\n",
    "\n",
    "    Argumentos:\n",
    "    dataframe (pd.DataFrame): El dataframe que contiene los datos.\n",
    "    target_col (str): Nombre de la columna objetivo para la clasificación. Valor por defecto es una cadena vacía.\n",
    "    columns (list): Lista de nombres de columnas a considerar. Si no se proporciona, se consideran todas las columnas numéricas. Valor por defecto es None.\n",
    "    pvalue (float): Nivel de significación para la prueba de ANOVA. Valor por defecto es 0.05.\n",
    "\n",
    "    Retorna:\n",
    "    list: Devuelve una lista de nombres de columnas que cumplen con el criterio de significación especificado.\n",
    "    \"\"\"\n",
    "    # Validar entradas\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        raise ValueError(\"dataframe debe ser un DataFrame de pandas\")\n",
    "    if not isinstance(target_col, str):\n",
    "        raise ValueError(\"target_col debe ser un string\")\n",
    "    if columns is not None and not all(isinstance(col, str) for col in columns):\n",
    "        raise ValueError(\"columns debe ser una lista de strings\")\n",
    "    if not isinstance(pvalue, (int, float)) or not (0 < pvalue < 1):\n",
    "        raise ValueError(\"pvalue debe ser un número entre 0 y 1\")\n",
    "    \n",
    "    # Si columns es None, igualar a las columnas numéricas del dataframe\n",
    "    if columns is None:\n",
    "        columns = dataframe.select_dtypes(include=['number']).columns.tolist()\n",
    "    else:\n",
    "        # Filtrar solo las columnas numéricas que están en la lista\n",
    "        columns = [col for col in columns if dataframe[col].dtype in ['float64', 'int64']]\n",
    "    \n",
    "    # Asegurarse de que target_col esté en el dataframe\n",
    "    if target_col and target_col not in dataframe.columns:\n",
    "        raise ValueError(f\"{target_col} no está en el dataframe\")\n",
    "    \n",
    "    # Filtrar columnas que cumplen el test de ANOVA\n",
    "    valid_columns = []\n",
    "    if target_col:\n",
    "        unique_classes = dataframe[target_col].unique()\n",
    "        for col in columns:\n",
    "            groups = [dataframe[dataframe[target_col] == cls][col].dropna() for cls in unique_classes]\n",
    "            if len(groups) > 1 and all(len(group) > 0 for group in groups):\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    f_val, p_val = f_oneway(*groups)\n",
    "                if p_val < pvalue:\n",
    "                    valid_columns.append(col)\n",
    "    else:\n",
    "        valid_columns = columns\n",
    "\n",
    "    # Si no hay columnas válidas, retornar una lista vacía\n",
    "    if not valid_columns:\n",
    "        return []\n",
    "\n",
    "    # Configuración de estilos\n",
    "    sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "    \n",
    "    # Crear pairplots agrupados\n",
    "    max_cols_per_plot = 5  # Máximo de columnas por plot\n",
    "    if target_col:\n",
    "        for i in range(0, len(valid_columns), max_cols_per_plot):\n",
    "            plot_columns = valid_columns[i:i+max_cols_per_plot]\n",
    "            if target_col not in plot_columns:\n",
    "                plot_columns.append(target_col)\n",
    "            \n",
    "            # Depuración: imprimir columnas y verificar contenido de subset_df\n",
    "            print(f\"Plot columns: {plot_columns}\")\n",
    "            print(f\"DataFrame shape: {dataframe.shape}\")\n",
    "            print(f\"DataFrame columns: {dataframe.columns.tolist()}\")\n",
    "            \n",
    "            g = sns.PairGrid(dataframe[plot_columns], hue=target_col)\n",
    "            g.map_diag(sns.histplot, kde=True, edgecolor='black', linewidth=0.5)\n",
    "            g.map_offdiag(sns.scatterplot, s=10, edgecolor=\"w\", linewidth=0.5)\n",
    "            g.add_legend()\n",
    "            plt.show()\n",
    "    else:\n",
    "        # Sin target_col, dividir en grupos de max_cols_per_plot\n",
    "        for i in range(0, len(valid_columns), max_cols_per_plot):\n",
    "            plot_columns = valid_columns[i:i+max_cols_per_plot]\n",
    "            \n",
    "            # Depuración: imprimir columnas y verificar contenido de subset_df\n",
    "            print(f\"Plot columns: {plot_columns}\")\n",
    "            print(f\"DataFrame shape: {dataframe.shape}\")\n",
    "            print(f\"DataFrame columns: {dataframe.columns.tolist()}\")\n",
    "\n",
    "            g = sns.PairGrid(dataframe[plot_columns])\n",
    "            g.map_diag(sns.histplot, kde=True, edgecolor='black', linewidth=0.5)\n",
    "            g.map_offdiag(sns.scatterplot, s=10, edgecolor=\"w\", linewidth=0.5)\n",
    "            plt.show()\n",
    "    \n",
    "    return valid_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

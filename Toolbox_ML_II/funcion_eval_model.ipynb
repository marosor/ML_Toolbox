{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion: eval_model\n",
    "\n",
    "Esta función debe recibir un target, unas predicciones para ese target, un argumento que determine si el problema es de regresión o clasificación y una lista de métricas:\n",
    "* Si el argumento dice que el problema es de regresión, la lista de métricas debe admitir las siguientes etiquetas RMSE, MAE, MAPE, GRAPH.\n",
    "* Si el argumento dice que el problema es de clasificación, la lista de métrica debe admitir, ACCURACY, PRECISION, RECALL, CLASS_REPORT, MATRIX, MATRIX_RECALL, MATRIX_PRED, PRECISION_X, RECALL_X. En el caso de las _X, X debe ser una etiqueta de alguna de las clases admitidas en el target.\n",
    "\n",
    "Funcionamiento:\n",
    "* Para cada etiqueta en la lista de métricas:\n",
    "- RMSE, debe printar por pantalla y devolver el RMSE de la predicción contra el target.\n",
    "- MAE, debe pintar por pantalla y devolver el MAE de la predicción contra el target. \n",
    "- MAPE, debe pintar por pantalla y devolver el MAPE de la predcción contra el target. Si el MAPE no se pudiera calcular la función debe avisar lanzando un error con un mensaje aclaratorio\n",
    "- GRAPH, la función debe pintar una gráfica comparativa (scatter plot) del target con la predicción\n",
    "- ACCURACY, pintará el accuracy del modelo contra target y lo retornará.\n",
    "- PRECISION, pintará la precision media contra target y la retornará.\n",
    "- RECALL, pintará la recall media contra target y la retornará.\n",
    "- CLASS_REPORT, mostrará el classification report por pantalla.\n",
    "- MATRIX, mostrará la matriz de confusión con los valores absolutos por casilla.\n",
    "- MATRIX_RECALL, mostrará la matriz de confusión con los valores normalizados según el recall de cada fila (si usas ConfussionMatrixDisplay esto se consigue con normalize = \"true\")\n",
    "- MATRIX_PRED, mostrará la matriz de confusión con los valores normalizados según las predicciones por columna (si usas ConfussionMatrixDisplay esto se consigue con normalize = \"pred\")\n",
    "- PRECISION_X, mostrará la precisión para la clase etiquetada con el valor que sustituya a X (ej. PRECISION_0, mostrará la precisión de la clase 0)\n",
    "- RECALL_X, mostrará el recall para la clase etiquetada co nel valor que sustituya a X (ej. RECALL_red, mostrará el recall de la clase etiquetada como \"red\")\n",
    "\n",
    "NOTA1: Como puede que la función devuelva varias métricas, debe hacerlo en una tupla en el orden de aparición de la métrica en la lista que se le pasa como argumento. Ejemplo si la lista de entrada es [\"GRAPH\",\"RMSE\",\"MAE\"], la fución pintará la comparativa, imprimirá el RMSE y el MAE (da igual que lo haga antes de dibujar la gráfica) y devolverá una tupla con el (RMSE,MAE) por ese orden.\n",
    "NOTA2: Una lista para clasificación puede contener varias PRECISION_X y RECALL_X, pej [\"PRECISION_red\",\"PRECISION_white\",\"RECALL_red\"] es una lista válida, tendrá que devolver la precisión de \"red\", la de \"white\" y el recall de \"red\". Si algunas de las etiquetas no existe debe arrojar ese error y detener el funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(target, predicciones, tipo_de_problema, metricas):\n",
    "\n",
    "    \"\"\"\n",
    "    Función que evalua un modelo de Machine Learning utilizando diferentes métricas para problemas de regresión o clasificación\n",
    "\n",
    "    Argumentos:\n",
    "    target (tipo array): Valores del target\n",
    "    predicciones (tipo array): Valores predichos por el modelo\n",
    "    tipo_de_problema (str): Puede ser de regresión o clasificación\n",
    "    metricas (list): Lista de métricas a calcular:\n",
    "                     Para problemas de regresión: \"RMSE\", \"MAE\", \"MAPE\", \"GRAPH\"\n",
    "                     Para problemas de clasificación: \"ACCURACY\", \"PRECISION\", \"RECALL\", \"CLASS_REPORT\", \"MATRIX\", \"MATRIX_RECALL\", \"MATRIX_PRED\", \"PRECISION_X\", \"RECALL_X\"\n",
    "\n",
    "    Retorna:\n",
    "    tupla: Devuelve una tupla con los resultados de las métricas especificadas\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Regresión\n",
    "\n",
    "    if tipo_de_problema == \"regresion\":\n",
    "\n",
    "        for metrica in metricas:\n",
    "            \n",
    "            if metrica == \"RMSE\":\n",
    "                rmse = np.sqrt(mean_squared_error(target, predicciones))\n",
    "                print(f\"RMSE: {rmse}\")\n",
    "                results.append(rmse)\n",
    "            \n",
    "            elif metrica == \"MAE\":\n",
    "                mae = mean_absolute_error(target, predicciones)\n",
    "                print(f\"MAE: {mae}\")\n",
    "                results.append(mae)\n",
    "\n",
    "            elif metrica == \"MAPE\":\n",
    "                try:\n",
    "                    mape = np.mean(np.abs((target - predicciones) / target)) * 100\n",
    "                    print(f\"MAPE: {mape}\")\n",
    "                    results.append(mape)\n",
    "                except ZeroDivisionError:\n",
    "                    raise ValueError(\"No se puede calcular el MAPE cuando hay valores en el target iguales a cero\")\n",
    "           \n",
    "            elif metrica == \"GRAPH\":\n",
    "                plt.scatter(target, predicciones)\n",
    "                plt.xlabel(\"Real\")\n",
    "                plt.ylabel(\"Predicción\")\n",
    "                plt.title(\"Gráfico de Dispersión: Valores reales VS Valores predichos\")\n",
    "                plt.show()\n",
    "\n",
    "     # Clasificación\n",
    "                \n",
    "    elif tipo_de_problema == \"clasificacion\":\n",
    "\n",
    "        for metrica in metricas:\n",
    "            \n",
    "            if metrica == \"ACCURACY\":\n",
    "                accuracy = accuracy_score(target, predicciones)\n",
    "                print(f\"Accuracy: {accuracy}\")\n",
    "                results.append(accuracy)\n",
    "\n",
    "            elif metrica == \"PRECISION\":\n",
    "                precision = precision_score(target, predicciones, average = \"macro\")\n",
    "                print(f\"Precision: {precision}\")\n",
    "                results.append(precision)\n",
    "\n",
    "            elif metrica == \"RECALL\":\n",
    "                recall = recall_score(target, predicciones, average = \"macro\")\n",
    "                print(f\"Recall: {recall}\")\n",
    "                results.append(recall)\n",
    "\n",
    "            elif metrica == \"CLASS_REPORT\":\n",
    "                print(\"Classification Report:\")\n",
    "                print(classification_report(target, predicciones))\n",
    "\n",
    "            elif metrica == \"MATRIX\":\n",
    "                print(\"Confusion Matrix (Absolute Values):\")\n",
    "                print(confusion_matrix(target, predicciones))\n",
    "\n",
    "            elif metrica == \"MATRIX_RECALL\":\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(target, predicciones))\n",
    "                disp.plot(normalize = \"true\")\n",
    "                plt.title(\"Confusion Matrix (Normalized by Recall)\")\n",
    "                plt.show()\n",
    "\n",
    "            elif metrica == \"MATRIX_PRED\":\n",
    "                disp = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(target, predicciones))\n",
    "                disp.plot(normalize = \"pred\")\n",
    "                plt.title(\"Confusion Matrix (Normalized by Prediction)\")\n",
    "                plt.show()\n",
    "\n",
    "            elif \"PRECISION_\" in metrica:\n",
    "                class_label = metrica.split(\"_\")[-1]\n",
    "                try:\n",
    "                    precision_class = precision_score(target, predicciones, labels = [class_label])\n",
    "                    print(f\"Precisión para la clase {class_label}: {precision_class}\")\n",
    "                    results.append(precision_class)\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"La clase {class_label} no está presente en las predicciones\")\n",
    "                \n",
    "            elif \"RECALL_\" in metrica:\n",
    "                class_label = metrica.split(\"_\")[-1]\n",
    "                try:\n",
    "                    recall_class = recall_score(target, predicciones, labels = [class_label])\n",
    "                    print(f\"Recall para la clase {class_label}: {recall_class}\")\n",
    "                    results.append(recall_class)\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"La clase {class_label} no está presente en las predicciones\")\n",
    "                \n",
    "    # Si no es regresión o clasificación\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"El tipo de problema debe ser de regresión o clasificación\")\n",
    "\n",
    "    return tuple(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
